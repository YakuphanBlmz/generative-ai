# Scalar Quantization in Vector Databases

[![English](https://img.shields.io/badge/View%20in-English-blue)](#english-content) [![Türkçe](https://img.shields.io/badge/Görüntüle-Türkçe-green)](#türkçe-içerik)

---
<a name="english-content"></a>
## English Content
### Table of Contents (EN)
- [1. Introduction](#1-introduction)
- [2. Vector Databases and Similarity Search](#2-vector-databases-and-similarity-search)
- [3. Understanding Scalar Quantization](#3-understanding-scalar-quantization)
  - [3.1. Mechanisms and Techniques](#31-mechanisms-and-techniques)
  - [3.2. Advantages of Scalar Quantization](#32-advantages-of-scalar-quantization)
  - [3.3. Disadvantages and Trade-offs](#33-disadvantages-and-trade-offs)
- [4. Integration with Vector Database Architectures](#4-integration-with-vector-database-architectures)
- [5. Code Example](#5-code-example)
- [6. Conclusion](#6-conclusion)

### 1. Introduction
The proliferation of **Generative AI** models and **large language models (LLMs)** has significantly amplified the demand for efficient **vector databases**. These specialized databases are designed to store and query high-dimensional **vector embeddings**, which represent the semantic meaning of data like text, images, or audio. The core operation in these systems is **similarity search**, finding vectors that are semantically close to a given query vector. As the scale of these vector datasets grows into billions, the memory and computational requirements become substantial. Storing billions of high-dimensional vectors, typically represented as 32-bit floating-point numbers (`float32`), demands vast amounts of storage and memory, impacting query latency and overall system cost.

To address these challenges, various **vector compression** techniques have emerged. One such technique, **Scalar Quantization (SQ)**, stands out for its simplicity and effectiveness in reducing the memory footprint of vector embeddings while preserving a reasonable level of search accuracy. This document will delve into the principles of Scalar Quantization, its mechanisms, advantages, disadvantages, and its practical application within the context of modern vector databases.

### 2. Vector Databases and Similarity Search
**Vector databases** are fundamental components in many AI applications, powering recommendation systems, semantic search, anomaly detection, and RAG (Retrieval Augmented Generation) architectures. They store vectors generated by **embedding models**, which transform complex data into a numerical representation where similar items have similar vectors.

The primary function of a vector database is to perform **similarity search** efficiently. Given a query vector, the database needs to quickly identify the *k* most similar vectors from its vast collection. This similarity is typically measured using distance metrics such as **cosine similarity** or **Euclidean distance**. Due to the high dimensionality of these vectors, exact nearest neighbor search is computationally prohibitive for large datasets. Consequently, **Approximate Nearest Neighbor (ANN)** algorithms, such as Hierarchical Navigable Small Worlds (**HNSW**) or Inverted File Index (**IVF**), are employed. These algorithms trade off some precision for significant speed improvements, but they still operate on the full-precision vectors, making memory consumption a persistent concern.

### 3. Understanding Scalar Quantization
**Scalar Quantization (SQ)** is a lossy compression technique that reduces the precision of individual components (scalars) within a vector. Instead of storing each scalar as a high-precision floating-point number (e.g., `float32`), SQ maps these values to a lower-precision format, typically 8-bit integers (`int8` or `uint8`). This process drastically reduces the memory required to store each vector, often by a factor of 4 (from 32 bits to 8 bits per component).

The core idea behind SQ is to represent a range of floating-point values with a smaller, fixed set of integer values. This is achieved by defining a scaling factor and an offset (often called the "zero point") for each vector or for the entire dataset.

#### 3.1. Mechanisms and Techniques
The most common approach to Scalar Quantization is **uniform quantization**, where the range of floating-point values is divided into equally sized bins.

Let's consider a vector component `x` (a `float32` value). To quantize `x` to an `int8` value (`x_q`), the following steps are typically involved:
1.  **Determine Min/Max Range:** Identify the minimum (`min_val`) and maximum (`max_val`) values for the entire vector dimension across the dataset, or for each individual vector. Using a per-vector min/max often yields better precision.
2.  **Calculate Scale Factor:** The scale factor (`s`) determines how a range of floating-point values maps to the integer range (e.g., `[-128, 127]` for `int8`).
    `s = (max_val - min_val) / (max_int_val - min_int_val)`
    Where `max_int_val` and `min_int_val` are the maximum and minimum values representable by the target integer type (e.g., 127 and -128 for signed 8-bit integers).
3.  **Calculate Zero Point:** The zero point (`z`) is an offset that ensures the floating-point zero (or another reference point) maps correctly to an integer value, often used to preserve zero.
    `z = min_int_val - (min_val / s)`
    This `z` is often rounded to the nearest integer.
4.  **Quantize:** Convert the `float32` value `x` to `int8` `x_q`:
    `x_q = round(x / s + z)`
    The `x_q` value is then clipped to ensure it stays within the target integer range (`min_int_val` to `max_int_val`).
5.  **De-quantize (for similarity calculation):** When performing similarity search, the quantized vectors need to be de-quantized back to an approximation of their original floating-point values to calculate distances.
    `x_deq = (x_q - z) * s`

This process effectively compresses the data while allowing for approximate calculations. Some advanced SQ techniques might use non-uniform quantization or learn the optimal quantization parameters (like min/max ranges) using clustering algorithms, but uniform quantization is the most straightforward and widely implemented.

#### 3.2. Advantages of Scalar Quantization
Scalar Quantization offers several significant benefits for vector databases:
*   **Memory Footprint Reduction:** This is the primary advantage. Reducing each `float32` component to an `int8` means a 4x reduction in memory consumption per vector. This translates directly to lower storage costs and the ability to fit vastly more vectors into RAM.
*   **Improved I/O Performance:** Less data needs to be read from disk or transferred over the network, leading to faster data loading and retrieval.
*   **Enhanced Cache Locality:** With smaller vector sizes, more vectors can fit into the CPU cache, reducing cache misses and speeding up computations during similarity search.
*   **Potential for Faster Computations:** While de-quantization is required for distance calculation, some specialized hardware or libraries can perform approximate distance calculations directly on quantized integers, which can be faster than floating-point operations. For example, dot product or L2 distance calculations can be optimized for integer arrays.

#### 3.3. Disadvantages and Trade-offs
Despite its advantages, Scalar Quantization is a **lossy compression** method, meaning it comes with inherent trade-offs:
*   **Precision Loss and Recall Impact:** The most significant drawback is the loss of precision. Mapping a continuous range of floating-point numbers to a discrete set of integers inevitably introduces quantization error. This error can subtly alter the relative distances between vectors, potentially leading to a decrease in the **recall** (the proportion of relevant items found) of similarity search results.
*   **Complexity in Tuning:** Choosing the appropriate bit-width (e.g., `int8`, `int16`) and the scope of quantization (per-vector, per-dimension, or global) requires careful consideration and experimentation. A lower bit-width offers greater compression but higher precision loss.
*   **Overhead for Quantization/De-quantization:** While storage is reduced, there's a computational overhead involved in quantizing vectors during insertion and de-quantizing them during query time for distance calculations. However, this overhead is often negligible compared to the benefits of reduced I/O and improved cache performance.

### 4. Integration with Vector Database Architectures
Scalar Quantization is typically applied to the vectors *before* they are indexed by ANN algorithms. For instance, when using HNSW, the graph structure is built using the full-precision vectors, but the actual data stored at each node (the vector embeddings themselves) can be quantized. During query time, the query vector might also be quantized, or full-precision and then used to compare against the de-quantized stored vectors.

Many vector database solutions (e.g., Milvus, Pinecone, Weaviate, Qdrant) offer Scalar Quantization as an optimization option. Users can often specify the quantization type and parameters when defining their collections or indices. The system then handles the quantization and de-quantization transparently. It's crucial for vector database implementations to provide mechanisms to evaluate the recall impact of SQ, allowing users to find the optimal balance between memory savings and search accuracy for their specific use case.

### 5. Code Example
Here’s a short Python snippet demonstrating a basic uniform scalar quantization of a `float32` vector to an `int8` vector.

```python
import numpy as np

def scalar_quantize_vector(vector_float32, num_bits=8):
    """
    Performs basic uniform scalar quantization on a single float32 vector
    to an int8 vector.

    Args:
        vector_float32 (np.array): A 1D numpy array of float32 values.
        num_bits (int): The number of bits for quantization (e.g., 8 for int8).

    Returns:
        tuple: A tuple containing:
            - np.array: The quantized int8 vector.
            - float: The scale factor used.
            - int: The zero point used.
    """
    if not isinstance(vector_float32, np.ndarray) or vector_float32.dtype != np.float32:
        raise ValueError("Input vector must be a numpy array of float32.")

    # Determine the range for the target integer type (e.g., int8: -128 to 127)
    if num_bits == 8:
        min_int_val, max_int_val = -128, 127
        int_dtype = np.int8
    else:
        raise NotImplementedError("Only 8-bit quantization is implemented in this example.")

    # Calculate min and max values across the input vector
    min_val = np.min(vector_float32)
    max_val = np.max(vector_float32)

    # Handle cases where min_val and max_val are identical to avoid division by zero
    if max_val == min_val:
        scale = 1.0 # Arbitrary scale, won't matter
        zero_point = min_int_val # Map all to min_int_val
    else:
        # Calculate scale factor
        scale = (max_val - min_val) / (max_int_val - min_int_val)
        # Calculate zero point
        # The zero point ensures `min_val` maps to `min_int_val`
        zero_point = min_int_val - round(min_val / scale)

    # Quantize the vector
    quantized_vector = np.round(vector_float32 / scale + zero_point).astype(int_dtype)

    # Clip values to ensure they stay within the target int range
    quantized_vector = np.clip(quantized_vector, min_int_val, max_int_val)

    return quantized_vector, scale, zero_point

def scalar_dequantize_vector(quantized_vector, scale, zero_point):
    """
    De-quantizes an int8 vector back to an approximate float32 vector.
    """
    dequantized_vector = (quantized_vector.astype(np.float32) - zero_point) * scale
    return dequantized_vector

# --- Example Usage ---
# Original float32 vector
original_vector = np.array([0.1, -0.5, 0.9, 0.0, 0.3, -0.2, 0.7, -0.8], dtype=np.float32)
print(f"Original vector (float32): {original_vector}")
print(f"Original vector size: {original_vector.nbytes} bytes")

# Quantize the vector to int8
quantized_vec, scale_factor, zero_point_val = scalar_quantize_vector(original_vector)
print(f"\nQuantized vector (int8): {quantized_vec}")
print(f"Quantized vector size: {quantized_vec.nbytes} bytes")
print(f"Scale factor: {scale_factor:.4f}, Zero point: {zero_point_val}")

# De-quantize the vector back to float32
dequantized_vec = scalar_dequantize_vector(quantized_vec, scale_factor, zero_point_val)
print(f"\nDe-quantized vector (float32): {dequantized_vec}")

# Check for approximation error
error = np.linalg.norm(original_vector - dequantized_vec)
print(f"Approximation error (L2 norm): {error:.4f}")


(End of code example section)
```

### 6. Conclusion
Scalar Quantization is a vital optimization technique for managing the scale and cost associated with high-dimensional vector embeddings in modern **vector databases**. By converting high-precision floating-point numbers to lower-precision integers, SQ significantly reduces memory consumption and improves I/O performance and cache locality. While it introduces a degree of precision loss that can impact search **recall**, the benefits in terms of resource efficiency often outweigh this trade-off, especially for very large datasets where exact precision is not always paramount. Effective implementation of SQ, coupled with careful evaluation of its impact on accuracy, allows developers to build more scalable, cost-effective, and performant AI systems powered by vector similarity search. The continuous advancements in quantization research aim to further minimize recall degradation while maximizing compression ratios, solidifying SQ's role as a cornerstone in efficient vector database design.

---
<br>

<a name="türkçe-içerik"></a>
## Vektör Veritabanlarında Skaler Kuantizasyon

[![English](https://img.shields.io/badge/View%20in-English-blue)](#english-content) [![Türkçe](https://img.shields.io/badge/Görüntüle-Türkçe-green)](#türkçe-içerik)

## Türkçe İçerik
### İçindekiler (TR)
- [1. Giriş](#1-giriş)
- [2. Vektör Veritabanları ve Benzerlik Araması](#2-vektör-veritabanları-ve-benzerlik-araması)
- [3. Skaler Kuantizasyonu Anlamak](#3-skaler-kuantizasyonu-anlamak)
  - [3.1. Mekanizmalar ve Teknikler](#31-mekanizmalar-ve-teknikler)
  - [3.2. Skaler Kuantizasyonun Avantajları](#32-skaler-kuantizasyonun-avantajları)
  - [3.3. Dezavantajları ve Dengelemeler](#33-dezavantajları-ve-dengelemeler)
- [4. Vektör Veritabanı Mimarilerine Entegrasyon](#4-vektör-veritabanı-mimarilerine-entegrasyon)
- [5. Kod Örneği](#5-kod-örneği)
- [6. Sonuç](#6-sonuç)

### 1. Giriş
**Üretken Yapay Zeka (Generative AI)** modellerinin ve **büyük dil modellerinin (LLM'ler)** yaygınlaşması, verimli **vektör veritabanlarına** olan talebi önemli ölçüde artırmıştır. Bu özel veritabanları, metin, görüntü veya ses gibi verilerin anlamsal anlamını temsil eden yüksek boyutlu **vektör gömülerini (vector embeddings)** depolamak ve sorgulamak için tasarlanmıştır. Bu sistemlerdeki temel işlem, verilen bir sorgu vektörüne anlamsal olarak yakın olan vektörleri bulma olan **benzerlik aramasıdır**. Bu vektör veri kümelerinin boyutu milyarlara ulaştıkça, bellek ve hesaplama gereksinimleri de önemli hale gelmektedir. Tipik olarak 32 bitlik kayan nokta sayıları (`float32`) olarak temsil edilen milyarlarca yüksek boyutlu vektörün depolanması, çok büyük miktarda depolama alanı ve bellek gerektirerek sorgu gecikmesini ve genel sistem maliyetini etkiler.

Bu zorlukları ele almak için çeşitli **vektör sıkıştırma** teknikleri ortaya çıkmıştır. Bu tekniklerden biri olan **Skaler Kuantizasyon (SQ)**, vektör gömülerinin bellek ayak izini azaltmada basitliği ve etkinliği ile öne çıkarken, makul bir arama doğruluğu seviyesini korur. Bu belge, Skaler Kuantizasyonun prensiplerini, mekanizmalarını, avantajlarını, dezavantajlarını ve modern vektör veritabanları bağlamındaki pratik uygulamasını inceleyecektir.

### 2. Vektör Veritabanları ve Benzerlik Araması
**Vektör veritabanları**, öneri sistemleri, anlamsal arama, anomali tespiti ve RAG (Retrieval Augmented Generation) mimarilerine güç veren birçok yapay zeka uygulamasının temel bileşenleridir. Bu veritabanları, karmaşık verileri benzer öğelerin benzer vektörlere sahip olduğu sayısal bir temsile dönüştüren **gömme modelleri** tarafından oluşturulan vektörleri depolarlar.

Bir vektör veritabanının birincil işlevi, **benzerlik aramasını** verimli bir şekilde gerçekleştirmektir. Verilen bir sorgu vektörü için veritabanının, geniş koleksiyonundan *k* en benzer vektörü hızla tanımlaması gerekir. Bu benzerlik genellikle **kosinüs benzerliği** veya **Öklid mesafesi** gibi mesafe metrikleri kullanılarak ölçülür. Bu vektörlerin yüksek boyutu nedeniyle, büyük veri kümeleri için tam en yakın komşu araması hesaplama açısından yasaklayıcıdır. Sonuç olarak, Hiyerarşik Gezinilebilir Küçük Dünyalar (**HNSW**) veya Ters Dosya Dizini (**IVF**) gibi **Yaklaşık En Yakın Komşu (ANN)** algoritmaları kullanılır. Bu algoritmalar, önemli hız iyileştirmeleri karşılığında bir miktar hassasiyetten ödün verirler, ancak yine de tam hassasiyetli vektörler üzerinde çalışırlar, bu da bellek tüketimini kalıcı bir endişe kaynağı yapar.

### 3. Skaler Kuantizasyonu Anlamak
**Skaler Kuantizasyon (SQ)**, bir vektördeki bireysel bileşenlerin (skalerlerin) hassasiyetini azaltan kayıplı bir sıkıştırma tekniğidir. Her bir skalerin yüksek hassasiyetli bir kayan nokta sayısı (örn. `float32`) olarak depolanması yerine, SQ bu değerleri daha düşük hassasiyetli bir formata, tipik olarak 8 bitlik tam sayılara (`int8` veya `uint8`) eşler. Bu işlem, her bir vektörü depolamak için gereken belleği önemli ölçüde, genellikle 4 kat (bileşen başına 32 bitten 8 bite) azaltır.

SQ'nun temel fikri, bir kayan nokta değerleri aralığını daha küçük, sabit bir tam sayı kümesiyle temsil etmektir. Bu, her vektör veya tüm veri kümesi için bir ölçeklendirme faktörü ve bir ofset (genellikle "sıfır noktası" olarak adlandırılır) tanımlanarak elde edilir.

#### 3.1. Mekanizmalar ve Teknikler
Skaler Kuantizasyon için en yaygın yaklaşım, kayan nokta değerleri aralığının eşit boyutlu bölmelere ayrıldığı **üniform kuantizasyondur**.

Bir vektör bileşeni `x` (bir `float32` değeri) düşünelim. `x` değerini bir `int8` değerine (`x_q`) kuantize etmek için tipik olarak aşağıdaki adımlar izlenir:
1.  **Min/Max Aralığını Belirleme:** Tüm veri kümesi boyunca veya her bir vektör için vektör boyutunun minimum (`min_val`) ve maksimum (`max_val`) değerlerini belirleyin. Vektör başına min/max kullanmak genellikle daha iyi hassasiyet sağlar.
2.  **Ölçek Faktörünü Hesaplama:** Ölçek faktörü (`s`), kayan nokta değerleri aralığının tam sayı aralığına (örn. `int8` için `[-128, 127]`) nasıl eşlendiğini belirler.
    `s = (max_val - min_val) / (max_int_val - min_int_val)`
    Burada `max_int_val` ve `min_int_val`, hedef tam sayı türü tarafından temsil edilebilen maksimum ve minimum değerlerdir (örn. işaretli 8 bit tam sayılar için 127 ve -128).
3.  **Sıfır Noktasını Hesaplama:** Sıfır noktası (`z`), kayan nokta sıfırının (veya başka bir referans noktasının) bir tam sayı değerine doğru şekilde eşlenmesini sağlayan bir ofsettir ve genellikle sıfırı korumak için kullanılır.
    `z = min_int_val - (min_val / s)`
    Bu `z` genellikle en yakın tam sayıya yuvarlanır.
4.  **Kuantize Etme:** `float32` değeri `x`i `int8` `x_q`ya dönüştürün:
    `x_q = round(x / s + z)`
    `x_q` değeri daha sonra hedef tam sayı aralığında (`min_int_val` ile `max_int_val` arasında) kalmasını sağlamak için kırpılır.
5.  **Kuantizasyonu Geri Alma (benzerlik hesaplaması için):** Benzerlik araması yaparken, kuantize edilmiş vektörlerin mesafeleri hesaplamak için orijinal kayan nokta değerlerinin bir yaklaşımına geri dönüştürülmesi gerekir.
    `x_deq = (x_q - z) * s`

Bu işlem, veriyi etkili bir şekilde sıkıştırırken yaklaşık hesaplamalara izin verir. Bazı gelişmiş SQ teknikleri, tekdüze olmayan kuantizasyon kullanabilir veya kümeleme algoritmaları kullanarak optimal kuantizasyon parametrelerini (min/max aralıkları gibi) öğrenebilir, ancak tekdüze kuantizasyon en basit ve en yaygın olarak uygulanan yöntemdir.

#### 3.2. Skaler Kuantizasyonun Avantajları
Skaler Kuantizasyon, vektör veritabanları için birçok önemli avantaj sunar:
*   **Bellek Ayak İzini Azaltma:** Bu, birincil avantajdır. Her `float32` bileşenini bir `int8`'e düşürmek, vektör başına bellek tüketiminde 4 kat azalma anlamına gelir. Bu, doğrudan daha düşük depolama maliyetlerine ve RAM'e çok daha fazla vektör sığdırma yeteneğine dönüşür.
*   **Gelişmiş G/Ç Performansı:** Diskten daha az veri okunması veya ağ üzerinden daha az veri aktarılması gerekir, bu da daha hızlı veri yükleme ve alma sağlar.
*   **Gelişmiş Önbellek Yerelliği:** Daha küçük vektör boyutlarıyla, CPU önbelleğine daha fazla vektör sığabilir, bu da önbellek kaçırmalarını azaltır ve benzerlik araması sırasında hesaplamaları hızlandırır.
*   **Daha Hızlı Hesaplama Potansiyeli:** Mesafe hesaplaması için kuantizasyonun geri alınması gerekse de, bazı özel donanımlar veya kütüphaneler, doğrudan kuantize edilmiş tam sayılar üzerinde yaklaşık mesafe hesaplamaları yapabilir, bu da kayan nokta işlemlerinden daha hızlı olabilir. Örneğin, nokta çarpım veya L2 mesafesi hesaplamaları tam sayı dizileri için optimize edilebilir.

#### 3.3. Dezavantajları ve Dengelemeler
Avantajlarına rağmen, Skaler Kuantizasyon **kayıplı bir sıkıştırma** yöntemidir, bu da doğal dengelemelerle birlikte gelir:
*   **Hassasiyet Kaybı ve Geri Çağırma Etkisi:** En önemli dezavantaj hassasiyet kaybıdır. Kayan nokta sayılarının sürekli bir aralığını ayrık bir tam sayı kümesine eşlemek, kaçınılmaz olarak kuantizasyon hatası getirir. Bu hata, vektörler arasındaki göreceli mesafeleri ince bir şekilde değiştirebilir ve benzerlik arama sonuçlarının **geri çağırma (recall)** oranında (bulunan ilgili öğelerin oranı) bir düşüşe yol açabilir.
*   **Ayarlamada Karmaşıklık:** Uygun bit genişliğini (örn. `int8`, `int16`) ve kuantizasyonun kapsamını (vektör başına, boyut başına veya genel) seçmek dikkatli değerlendirme ve deneme gerektirir. Daha düşük bir bit genişliği daha fazla sıkıştırma sağlarken, daha yüksek hassasiyet kaybına yol açar.
*   **Kuantizasyon/De-kuantizasyon İçin Ek Yük:** Depolama alanı azalırken, ekleme sırasında vektörleri kuantize etmek ve sorgu zamanında mesafe hesaplamaları için bunları geri kuantize etmekle ilgili bir hesaplama yükü vardır. Ancak, bu ek yük genellikle azaltılmış G/Ç ve geliştirilmiş önbellek performansının faydalarına kıyasla ihmal edilebilir düzeydedir.

### 4. Vektör Veritabanı Mimarilerine Entegrasyon
Skaler Kuantizasyon, ANN algoritmaları tarafından dizinlenmeden *önce* vektörlere uygulanır. Örneğin, HNSW kullanıldığında, grafik yapısı tam hassasiyetli vektörler kullanılarak oluşturulur, ancak her düğümde depolanan gerçek veriler (vektör gömülerinin kendileri) kuantize edilebilir. Sorgu zamanında, sorgu vektörü de kuantize edilebilir veya tam hassasiyetli olarak depolanmış kuantize edilmiş vektörlerle karşılaştırmak için kullanılabilir.

Birçok vektör veritabanı çözümü (örn. Milvus, Pinecone, Weaviate, Qdrant) Skaler Kuantizasyonu bir optimizasyon seçeneği olarak sunar. Kullanıcılar genellikle koleksiyonlarını veya dizinlerini tanımlarken kuantizasyon türünü ve parametrelerini belirleyebilirler. Sistem daha sonra kuantizasyon ve de-kuantizasyonu şeffaf bir şekilde yönetir. Vektör veritabanı uygulamalarının, SQ'nun geri çağırma üzerindeki etkisini değerlendirmek için mekanizmalar sağlaması çok önemlidir, bu da kullanıcıların kendi özel kullanım durumları için bellek tasarrufu ile arama doğruluğu arasında optimal dengeyi bulmalarını sağlar.

### 5. Kod Örneği
İşte bir `float32` vektörünü `int8` vektörüne temel tekdüze skaler kuantizasyonunu gösteren kısa bir Python kod parçacığı.

```python
import numpy as np

def scalar_quantize_vector(vector_float32, num_bits=8):
    """
    Tek bir float32 vektörü üzerinde temel tekdüze skaler kuantizasyonunu
    int8 vektörüne dönüştürür.

    Argümanlar:
        vector_float32 (np.array): float32 değerlerinden oluşan 1 boyutlu bir numpy dizisi.
        num_bits (int): Kuantizasyon için bit sayısı (örn. int8 için 8).

    Döndürür:
        tuple: Aşağıdakileri içeren bir tuple:
            - np.array: Kuantize edilmiş int8 vektörü.
            - float: Kullanılan ölçek faktörü.
            - int: Kullanılan sıfır noktası.
    """
    if not isinstance(vector_float32, np.ndarray) or vector_float32.dtype != np.float32:
        raise ValueError("Giriş vektörü, float32 türünde bir numpy dizisi olmalıdır.")

    # Hedef tam sayı türü için aralığı belirle (örn. int8: -128'den 127'ye)
    if num_bits == 8:
        min_int_val, max_int_val = -128, 127
        int_dtype = np.int8
    else:
        raise NotImplementedError("Bu örnekte sadece 8 bit kuantizasyon uygulanmıştır.")

    # Giriş vektöründeki min ve max değerlerini hesapla
    min_val = np.min(vector_float32)
    max_val = np.max(vector_float32)

    # min_val ve max_val'ın aynı olduğu durumları ele al, sıfıra bölmeyi önlemek için
    if max_val == min_val:
        scale = 1.0 # Keyfi ölçek, önemli değil
        zero_point = min_int_val # Hepsini min_int_val'a eşle
    else:
        # Ölçek faktörünü hesapla
        scale = (max_val - min_val) / (max_int_val - min_int_val)
        # Sıfır noktasını hesapla
        # Sıfır noktası, `min_val`'ın `min_int_val`'a doğru şekilde eşlenmesini sağlar
        zero_point = min_int_val - round(min_val / scale)

    # Vektörü kuantize et
    quantized_vector = np.round(vector_float32 / scale + zero_point).astype(int_dtype)

    # Değerleri hedef tam sayı aralığında kalmasını sağlamak için kırp
    quantized_vector = np.clip(quantized_vector, min_int_val, max_int_val)

    return quantized_vector, scale, zero_point

def scalar_dequantize_vector(quantized_vector, scale, zero_point):
    """
    Bir int8 vektörünü yaklaşık bir float32 vektörüne geri dönüştürür (de-kuantize eder).
    """
    dequantized_vector = (quantized_vector.astype(np.float32) - zero_point) * scale
    return dequantized_vector

# --- Örnek Kullanım ---
# Orijinal float32 vektörü
original_vector = np.array([0.1, -0.5, 0.9, 0.0, 0.3, -0.2, 0.7, -0.8], dtype=np.float32)
print(f"Orijinal vektör (float32): {original_vector}")
print(f"Orijinal vektör boyutu: {original_vector.nbytes} bayt")

# Vektörü int8'e kuantize et
quantized_vec, scale_factor, zero_point_val = scalar_quantize_vector(original_vector)
print(f"\nKuantize edilmiş vektör (int8): {quantized_vec}")
print(f"Kuantize edilmiş vektör boyutu: {quantized_vec.nbytes} bayt")
print(f"Ölçek faktörü: {scale_factor:.4f}, Sıfır noktası: {zero_point_val}")

# Vektörü float32'ye geri dönüştür (de-kuantize et)
dequantized_vec = scalar_dequantize_vector(quantized_vec, scale_factor, zero_point_val)
print(f"\nDe-kuantize edilmiş vektör (float32): {dequantized_vec}")

# Yaklaşım hatasını kontrol et
error = np.linalg.norm(original_vector - dequantized_vec)
print(f"Yaklaşım hatası (L2 normu): {error:.4f}")


(Kod örneği bölümünün sonu)
```

### 6. Sonuç
Skaler Kuantizasyon, modern **vektör veritabanlarındaki** yüksek boyutlu vektör gömüleriyle ilişkili ölçeği ve maliyeti yönetmek için hayati bir optimizasyon tekniğidir. Yüksek hassasiyetli kayan nokta sayılarını daha düşük hassasiyetli tam sayılara dönüştürerek, SQ bellek tüketimini önemli ölçüde azaltır ve G/Ç performansını ve önbellek yerelliğini iyileştirir. Arama **geri çağırmasını** etkileyebilecek bir hassasiyet kaybı derecesi getirse de, kaynak verimliliği açısından sağladığı faydalar genellikle bu dengelemeyi aşar, özellikle tam hassasiyetin her zaman öncelikli olmadığı çok büyük veri kümeleri için. SQ'nun etkin bir şekilde uygulanması, doğruluk üzerindeki etkisinin dikkatli bir şekilde değerlendirilmesiyle birleştiğinde, geliştiricilerin vektör benzerlik aramasıyla desteklenen daha ölçeklenebilir, uygun maliyetli ve yüksek performanslı yapay zeka sistemleri oluşturmasına olanak tanır. Kuantizasyon araştırmalarındaki sürekli ilerlemeler, sıkıştırma oranlarını en üst düzeye çıkarırken geri çağırma düşüşünü daha da en aza indirmeyi hedeflemekte ve SQ'nun verimli vektör veritabanı tasarımında bir köşe taşı rolünü sağlamlaştırmaktadır.

